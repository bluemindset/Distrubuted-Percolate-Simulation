
Two-dimensional domain de-composition

The serial version of the program uses a static L x L map for solving the problem.

For solving the problem, two-dimensional domain decomposition was used. A two-dimensional virtual topology on an MPI communicator helped with the optimization of communication. Each process is assigned a 2D small map, a subset of the map. (dims: M x N, where L>M & L>N for p >1).

Memory Allocation is dynamic for all the grids inside the program.
Therefore the program is memory efficient and allocates only the memory it requires for running the simulation. The generated grids on runtime are map, smallmap, old smallmap and new smallmap. The first have dimensions of N x N, while othe M x N
The user can run the parallel version of percolate by entering the suitable parameters in a configuration file and run a script.
The inputs are check for validation and that the parameters do not exceed boundaries. 
 
The master process (rank:0)  

Scatter & Gather
Scatter is implemented using Isend() and Irecv(). The map is sliced using MPI_Dims_create dimensions and each process gets a precise proportion of the map. The data type that is passed in the MPI communicator is a vector that can be represented as a single column.  For each slice, N columns are distributed by the master process to the corresponding worker processes (not broadcasted). The column has a size of one in width and M in length, where M is the particular length of the small-map that the process holds.  As stated before, each M and N can be different, so the vector that is sent has to be configured with the correct length. In comparison with sending all the columns of the small map, rather than fitting in one message, might have more communication overhead, but the messages are smaller in size. Moreover, greater control is provided. The current implementation can handle the uneven division of the grid and processes. If a process has a smaller M than the other ones, the number of columns that are sent is decreased. Gather works the same as scatter, with the difference that each column that is sent from the workers to the master process has to be placed in the correct position of the map.

Update Iterative Loop
Firstly, inside the update loop, the halo exchange takes place. Halos are shallow, which means that only two rows and two columns are passed to the neighbouring process. Halo exchange is done using non-blocking communication (Isend() & Irecv). When these are issued, there is no immediate waiting, but the inner cells (cells that are not in halos) are updated. This enables work to be done without waiting for halos to complete their exchange. After updating the inner cells, the waiting occurs using Waitall function(). After the exchange is done, the outer cells (halo cells) are updated. Furthermore, when all updates complete, the sum of the small-map array is measured. Then, an MPI_Reduce is issued to the master process. The master process gathers all the local sums of each proccess, adds them into global sum and finds the average. For every one hundred steps, the average is  printed along with the number of changes. Additionally, the stopping criterion differs than the serial's version. In the serial version, a max step is declared and if only is exceeded the iterative loop stops, which it is inefficient and does not allow the executive time of a single step to be measured. For a proper terminating, MPI_Allreduce is applied. Every process sends individually the number of the update's changes that happened on a single iteration (nchanges). The loop 

